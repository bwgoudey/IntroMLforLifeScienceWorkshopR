{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72p6SNZh-MEw"
      },
      "source": [
        "# Workshop -- Machine learning in life sciences\n",
        "### What is it, when should it be used and how to avoid common pitfalls\n",
        "\n",
        "**Author:** Benjamin Goudey, Research Fellow in Florey Department of Neuroscience and Mental Health at The University of Melbourne\n",
        "**Last updated:** 04/6/2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNVNKrnA9hsW"
      },
      "source": [
        "# Introduction\n",
        "Welcome to the workshop! This notebook accompanies the workshop *Applying machine learning in life sciences: what does it mean and how to avoid common traps.*\n",
        "\n",
        "The notebook focuses on the problem of predicting diabetes from a few clinical and blood measurements, as well as several \"noise\" variables.\n",
        "\n",
        "The notebook is split into into four sections.\n",
        "\n",
        "0. Welcome and setup\n",
        "1. Exploring the data and fitting a model and measuring performance\n",
        "2. Pitfall 1: Evaluation frameworks and generalisation\n",
        "3.  Pitfall 2: Selecting features and model parameters\n",
        "\n",
        "There will be a number of models, measures and algorithms that will be used and will be briefly explained in the accompanying tutorial but will not be covered in detail. The tidymodels documentation will be valuable here (https://www.tidymodels.org/find/all/)\n",
        "\n",
        "The notebook assumes familiarity with R and tidyverse and a passing familiarity with the ggplot packages. But even if you don't have this, the idea is that this notebook should help you get an idea of some of the concepts around machine learning and may be a useful resource for you at some stage.\n",
        "\n",
        "**Please note:** the expectation is you should be able to follow along rather than write this code from scratch. You should be able to run each cell in the notebook to get an output and then comments should direct you to indicate which parameters to change. If you get stuck, let us know!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uMExoTq-Jum"
      },
      "source": [
        "## Section 0: Package loading and data set up\n",
        "Don't worry too much about the code in this section. We load in the necessary packages and then there are a number of functions to load in the data or plot outputs. The details of these are mostly not needed for this workshop and I'll step through in more detail when they are needed.\n",
        "\n",
        "**Important:** I've found that the code below can take quite a long time to be loaded the first time you run this (**~20 minutes**)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab set up\n",
        "#\n",
        "# Because we are using Google Colab, I've stored the environment that we need\n",
        "# for this tutorial. This code downloads the environment and loads it again.\n",
        "#\n",
        "# Will take around 1 min to run.\n",
        "download.file(url = \"https://unimelbcloud-my.sharepoint.com/:u:/g/personal/ben_goudey_florey_edu_au/EY5qSG0Lk05OsjZsQNPfaWwBUKembT_JtIE_aOu-CoFTqg?download=1\", destfile = 'IntroToMLEnv_20240607.zip')\n",
        "\n",
        "unzip(\"IntroToMLEnv_20240607.zip\", overwrite=T, exdir=\"/usr/local/lib/R\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9mzM2ws9Gxq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# If you want to download this notebook and run it locally, run this cell instead\n",
        "# of the first one.\n",
        "\n",
        "#\n",
        "#  if (!requireNamespace(\"rpart.plot\", quietly = TRUE)) {\n",
        "#    install.packages(\"rpart.plot\")\n",
        "#  }\n",
        "#\n",
        "#  # if the version of ggplot installed is less than 3.5, install a new version\n",
        "#  # from github\n",
        "#  if (as.numeric(gsub('^([0-9]\\\\.[0-9]).*', '\\\\1', packageVersion('ggplot2')))<3.5) {\n",
        "#    remotes::install_github('tidyverse/ggplot2')\n",
        "#  }\n",
        "#\n",
        "#  # If pROC, a packaage for ROC curves is not installed, install it.\n",
        "#  if (!requireNamespace(\"pROC\", quietly = TRUE)) {\n",
        "#    remotes::install_github(\"xrobin/pROC\")\n",
        "#  }\n",
        "#\n",
        "#  # If DataExplorer, a packaage for summarising data frames, is not installed, install it.\n",
        "#  if (!requireNamespace(\"DataExplorer\", quietly = TRUE)) {\n",
        "#    install.packages(\"DataExplorer\")\n",
        "#  }\n",
        "#\n",
        "#  # If tidymodels, the framework for machine learning, id not installed, install it.\n",
        "#  if (!requireNamespace(\"tidymodels\", quietly = TRUE)) {\n",
        "#      install.packages(\"tidymodels\")\n",
        "#  }\n",
        "#}\n",
        "#"
      ],
      "metadata": {
        "id": "v8sgDkSFH2Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7VW6DR3pc9C"
      },
      "outputs": [],
      "source": [
        "# Load packages\n",
        "#\n",
        "library(rpart.plot)\n",
        "library(tidyverse)\n",
        "library(MASS)  # for mvnorm (multivariate normal data generation)\n",
        "library(scales)  # for rescaling data'\n",
        "library(remotes)\n",
        "library(tidymodels)\n",
        "library(DataExplorer)\n",
        "library(pROC)\n",
        "\n",
        "install.packages(c(\"randomForest\", \"ranger\", \"glmnet\"))\n",
        "# Also download a smaller helper file\n",
        "# You'll need to hit the refresh button to see this.\n",
        "download.file(url='https://raw.githubusercontent.com/bwgoudey/IntroMLforLifeScienceWorkshopR/main/helpers.R', destfile = 'helpers.R'  )\n",
        "\n",
        "## Read in a set of helper functions\n",
        "# Normally you'd just hve these locally but because we are using Google Colab, I found it\n",
        "# easier to read these in directly from github.\n",
        "devtools::source_url('https://raw.githubusercontent.com/bwgoudey/IntroMLforLifeScienceWorkshopR/main/helpers.R')\n",
        "#source('helpers.R')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeB_lOJL_c7b"
      },
      "source": [
        "## 0.2 Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VrF9MfVpyis"
      },
      "outputs": [],
      "source": [
        "# Read the dataset straight from github\n",
        "set.seed(1)\n",
        "diabetes_df <- read.csv(url(\"https://raw.githubusercontent.com/bwgoudey/IntroMLforLifeScienceWorkshopR/main/RC_health_data_n2000.csv\"))\n",
        "diabetes_df = diabetes_df %>%\n",
        "  dplyr::select(-id) %>%\n",
        "    mutate(diabetes=factor(diabetes))\n",
        "train_test_split=initial_split(diabetes_df, prop = 0.8)\n",
        "diabetes_train_df = training(train_test_split)\n",
        "diabetes_test_df= testing(train_test_split)\n",
        "diabetes_study2_df=diabetes_df %>% group_by(diabetes) %>% slice_head(n=20) %>% ungroup()\n",
        "\n",
        "cat(sprintf(\"Number of individuals in training data: %s\\n\", nrow(diabetes_train_df)))\n",
        "cat(sprintf(\"Number of individuals in test data: %s\\n\", nrow(diabetes_test_df)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9LtCjOBvA4f"
      },
      "source": [
        "## 0.3 Plotting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc_auc <- function(yp_train_df, yp_test_df, y_train, y_test, label, base_size=25) {\n",
        "\n",
        "  preds <- yp_train_df %>%\n",
        "      bind_cols(y_train) %>%\n",
        "      mutate(subset=\"train\") %>%\n",
        "    rbind(\n",
        "      yp_test_df %>%\n",
        "      bind_cols(y_test) %>%\n",
        "      mutate(subset=\"test\")\n",
        "    ) %>%\n",
        "  mutate(subset=factor(subset, c(\"train\", \"test\")))\n",
        "\n",
        "  auc_df <- preds %>%\n",
        "    group_by(subset) %>%\n",
        "      yardstick::roc_auc(truth = diabetes, .pred_0)  %>%\n",
        "        rename(auc=.estimate)\n",
        "\n",
        "  roc_curve_df <- preds %>%\n",
        "    group_by(subset) %>%\n",
        "      yardstick::roc_curve(truth = diabetes, .pred_0)\n",
        "\n",
        "  metrics_df = roc_curve_df %>%\n",
        "    left_join(auc_df, by='subset')\n",
        "\n",
        "  # Plot training and external validation ROC curves\n",
        "  metrics_df %>%\n",
        "    ggplot(aes(x=1-specificity, y=sensitivity, color=subset)) +\n",
        "    geom_line() +\n",
        "    labs(title = paste(label, \"ROC Curve\"),\n",
        "         x = \"False Positive Rate (1-Specificity)\",\n",
        "         y = \"True Positive Rate (Sensitivity)\") +\n",
        "    theme_light(base_size = base_size) +\n",
        "    geom_text(\n",
        "      aes(x = 0.6, y = 0.3, label = paste(\"AUC =\", round(auc,3))),\n",
        "      size = base_size/2,  inherit.aes = FALSE\n",
        "    ) +  facet_wrap(~subset, nrow=1) + theme(legend.position = 'none')\n",
        "}\n"
      ],
      "metadata": {
        "id": "Hw7h6mcDLDb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-I8yWQlBZbm"
      },
      "source": [
        "# Session 1: Exploring the data and fitting a basic model\n",
        "\n",
        "Here, we will explore a given dataset related to diabetes and fit a basic model using the scikit-learn package.\n",
        "\n",
        "In particular, we aim to\n",
        " - understand the dataset, its variables and their relationship.\n",
        " - introduce the pandas-profiling and scikit-learn packages.\n",
        " - demonstrate how to fit a model using sklearn and look at the outputs.\n",
        "\n",
        "## Background: Dataset\n",
        "\n",
        "\n",
        "**Ben to update this, we are additionally using a very similar dataset but derived from a much larger place. Will allow us to explore internal and external validation strategies***\n",
        "\n",
        "We will be making use of a real dataset of 442 diabetes patients as well as a simulated dataset derived from this data. This is relatively small in the machine learning space but is common (or even large) for many clinical/bioinformatics analyses.\n",
        "\n",
        "10 attributes have been measured, with variable names and descriptions given below:\n",
        "- *age*: age in years\n",
        "- *sex*: biological sex of the participant\n",
        "- *bmi*: body mass index\n",
        "- *bp*: average blood pressure\n",
        "- *tc*: total serum cholesterol\n",
        "- *ldl*: low-density lipoproteins\n",
        "- *hdl*: high-density lipoproteins\n",
        "- *tch*: total cholesterol / HDL\n",
        "- *ltg*: possibly log of serum triglycerides level\n",
        "- *glu*: blood sugar level\n",
        "\n",
        "Here, we predict a binary target indicating progression of diabetes after one year or not (1 or 0). This is derived from \"a quantitative measure of disease progression one year after baseline\" (Efron et al. (2004)) though it is unclear exactly what this measurement is. I've threshold this value at 100.\n",
        "\n",
        "\n",
        "We derive a simulated dataset from the original dataset which we treat as an external replication cohort. By default there are 1000 samples in this simulated data.\n",
        "\n",
        "### Analysis aim\n",
        "The analysis goals from this dataset are typical of a predictive task in this area:\n",
        ">Two hopes were evident [from the data], that the model would produce accurate baseline predictions of response for future patients and that the form of the model would suggest which covariates were important factors in disease progression.\n",
        "\n",
        "I'm going to assume a more specific question **\"do blood serum markers help predict diabetes progression beyond age, sex, bmi and blood glucose?\"**. Now we have a specific baseline we can evaluate against.\n",
        "\n",
        "Further information is available at https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset and data is taken from the original paper https://tibshirani.su.domains/ftp/lars.pdf.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjDj7JEbwjnc"
      },
      "source": [
        "### 1.1 Basic data exploration\n",
        "\n",
        "The first step in any modelling some **exploratory data analysis (EDA)** to get a feeling for the data that you will be exploring. This can help to identify variables that are highly skewed, have strong outlier or which have been recorded in an unexpected manner. They can also help give some expectations around the difficulty of the machine learning task that you want to explore.\n",
        "\n",
        "While we have a description of the fields, understanding the relationships between individual features and their relationship with the outcome of interest is informative for helping to understand downstream.\n",
        "\n",
        "The DataExplorer package takes a data frame and gives a report that has a number of summary statistics of the variables in the dataset.\n",
        "\n",
        "\n",
        "**Examine the following:**\n",
        "\n",
        "1. What are the different types of features - which are numerical? which are categorical? are any unclear?\n",
        "2. Which features are correlated with each other? How strong are these?\n",
        "3. Is there anything unexpected about the data?\n",
        "4. Is there any missing data?\n",
        "5. What does the target variable look like? What is its distribution? Are there any obvious relationships?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "options(repr.plot.width = 10, repr.plot.height = 10, repr.plot.res = 100)\n",
        "#DataExplorer::create_report(diabetes_train_df)\n",
        "plot_intro(diabetes_df,\n",
        "           title = \"Overview of Diabetes Data\",\n",
        "           ggtheme = theme_minimal(base_size = 20),\n",
        "                )\n",
        "plot_missing(diabetes_df,\n",
        "           title = \"Missingness in Diabetes Data\",\n",
        "           ggtheme = theme_minimal(base_size = 20))\n",
        "\n",
        "plot_bar(diabetes_df, by=\"diabetes\",\n",
        "           ggtheme = theme_minimal(base_size = 20))\n",
        "\n",
        "plot_histogram(diabetes_df,\n",
        "           title = \"Frequency of categories for Categorical data\",\n",
        "           ggtheme = theme_minimal(base_size = 20))\n",
        "\n",
        "plot_correlation(diabetes_df, maxcat = 5L,\n",
        "           title = \"Correlation heatmap\",\n",
        "           ggtheme = theme_minimal(base_size = 20))"
      ],
      "metadata": {
        "id": "ERCsYlOla0Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMd8II1iyAZ4"
      },
      "source": [
        "## 1.3 Fitting a model to the data\n",
        "\n",
        "Lets fit a simple logistic regression to the data and look at how well it makes predictions on the data. The code below will fit a simple logistic regression , using only a single predictor (bmi), to allow for simple plots of the fit, and then using all available features.\n",
        "\n",
        "We begin by plotting the data, the model fit and some classic summary statistics.\n",
        "\n",
        "*Questions*:\n",
        " - Which features are the most predictive?\n",
        " - How much improvement to you get if you combined features compared to a model based on individual features?\n",
        " - Try regenerating the dataset with more or less noise. What happens to prediction accuracy as you add more noisy variables?\n",
        "\n",
        " Note: the data has a bunch of variables called noise_<number> e.g noise_1, noise_2 etc. These are just randomly generated numbers. But in real life these types of variables do exist - they are essentially any variable that is unrelated to the thing we are trying to predict. In this data, we explicitly know these noise variables but in your own datasets (and for the other variables in the diabetes dataset), we have no idea which variables are related and which are not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTacJgQ2p3Ys"
      },
      "outputs": [],
      "source": [
        "options(repr.plot.width = 12, repr.plot.height = 10, repr.plot.res = 100)\n",
        "\n",
        "# Setting up the model specification\n",
        "logit_spec <- logistic_reg(mode = \"classification\", penalty = 0, engine = \"glm\")\n",
        "\n",
        "# Setting up the workflow\n",
        "workflow <- workflow() %>%\n",
        "  add_model(logit_spec) %>%\n",
        "  add_formula(diabetes ~ .) # Any variable on the left-hand side of the tilde (~) is considered the model outcome (here, arr_delay). On the right-hand side of the tilde are the predictors. Variables may be listed by name, or you can use the dot (.) to indicate all other variables as predictors.\n",
        "\n",
        "# Fitting the model\n",
        "model_fit <- workflow %>%\n",
        "         fit(data = diabetes_train_df)\n",
        "\n",
        "# Make predictions\n",
        "diabetes_train_preds <- predict(model_fit, diabetes_train_df, type = \"prob\")\n",
        "diabetes_test_preds  <- predict(model_fit, diabetes_test_df, type = \"prob\")\n",
        "\n",
        "# Pull out the actual labels\n",
        "diabetes_train_labels <- diabetes_train_df %>% dplyr::select(diabetes)\n",
        "diabetes_test_labels  <- diabetes_test_df %>% dplyr::select(diabetes)\n",
        "\n",
        "\n",
        "# Now plot the ROC curves and their AUCs\n",
        "plot_roc_auc(diabetes_train_preds, diabetes_test_preds,\n",
        "             diabetes_train_labels, diabetes_test_labels,\n",
        "             label=\"Diabetes - Logistic Regression\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDfVKPPjMCvC"
      },
      "source": [
        "## 1.4 Examine top features\n",
        "Logistic regression provides an interpretable model. To see which features are most important, we can look at the coefficients of each variable. Larger absolute values implies more impact in the predictions.\n",
        "\n",
        "\n",
        "**Examine the following:**\n",
        "\n",
        "1. Do any noise variables make it into the top 10?\n",
        "2. What if you generate lots of noise variables (>1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq5lCmsQMGcz"
      },
      "outputs": [],
      "source": [
        "model_fit$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NeoVO2jMHAH"
      },
      "source": [
        "\n",
        "# Session 2: More models and metrics in `tidymodels`\n",
        "\n",
        "In the previous example, we fit a logistic regression model to the given dataset and examine its performance using AUC. However, in many studies where we are looking to create a predictive model, we will be interested in creating multiple models based on different underlying algorithms and possibly evaluating them based on different criteria. Here, we demonstrate how these different models and metrics can be called and provide a few examples with our diabetes dataset as to the information you get.      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwQYEj58MRfl"
      },
      "source": [
        "## 2.1 Exploring different models\n",
        "The sklearn has quite a standardised interface for fitting and applying different models. In particular clf.fit(X, y) and clf.predict_proba(X) can be used to fit a model and then extract the probabilities of the predicted classes. Comparable functions exist when looking at continuous or multi-label outcomes.\n",
        "\n",
        "These standardised interfaces allow us to easily explore the impact of different classifiers for a problem. UNderstanding the different assumptions and methods is beyond this workshop. However, we can explore how this is done and talk through the impact of diffent classifiers.\n",
        "\n",
        "\n",
        "**Examine the following:**\n",
        "\n",
        "1. How well do different classifiers perform on the given dataset? Which models maximise performance on the training set? What does external performance look like?\n",
        "2. Are there noticable timing differences?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzHO3rTRMeWP"
      },
      "outputs": [],
      "source": [
        "# Define the models\n",
        "models=list()\n",
        "models[['log_reg']] = logistic_reg(mode = \"classification\", penalty = 0) %>%\n",
        "                             set_engine(\"glm\") %>%\n",
        "                             set_mode(\"classification\")\n",
        "\n",
        "models[['lasso']] = logistic_reg(penalty = 0.1, mixture = 1) %>%\n",
        "                             set_engine(\"glmnet\") %>%\n",
        "                             set_mode(\"classification\")\n",
        "\n",
        "models[['rf']] = rand_forest(mtry = 2, trees = 500, min_n = 3) %>%\n",
        "                             set_engine(\"randomForest\") %>%\n",
        "                             set_mode(\"classification\")\n",
        "\n",
        "models[['decision_tree']]= decision_tree(tree_depth = 5) %>%\n",
        "                             set_engine(\"rpart\") %>%\n",
        "                             set_mode(\"classification\")\n",
        "\n",
        "# Select the model of interest\n",
        "current_model=models[[\"decision_tree\"]] %>% set_mode(\"classification\")\n",
        "\n",
        "# Fit the model\n",
        "model_fit <- current_model %>%\n",
        "  fit(diabetes ~ ., data = diabetes_train_df)\n",
        "\n",
        "# Make predictions\n",
        "diabetes_train_preds <- predict(model_fit, diabetes_train_df, type = \"prob\")\n",
        "diabetes_test_preds  <- predict(model_fit, diabetes_test_df, type = \"prob\")\n",
        "\n",
        "# Pull out the actual labels\n",
        "diabetes_train_labels <- diabetes_train_df %>% dplyr::select(diabetes)\n",
        "diabetes_test_labels  <- diabetes_test_df %>% dplyr::select(diabetes)\n",
        "\n",
        "\n",
        "# Now plot the ROC curves and their AUCs\n",
        "plot_roc_auc(diabetes_train_preds, diabetes_test_preds,\n",
        "             diabetes_train_labels, diabetes_test_labels,\n",
        "             label=\"Diabetes - Logistic Regression\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ8Mcxh2Mene"
      },
      "source": [
        "## 2.2 Example of a specific model output\n",
        "Different machine learning methods have very different underlying algorithms and can produce very different outputs.\n",
        "\n",
        "Below we take an example at a unique output, that of a decision tree, to peek into how it works. Here, the model produces a sort of flowchart for how a sample should be classifier, based on a series of binary decisions. We visualise this model below where each node shows\n",
        "1. A variable chosen to make a decision and a threshold for which side of the subtree we go down\n",
        "2. gini value, a measure of how well the given split separates classes (lower is more discriminative)\n",
        "3. samples is the number of samples in a node in the training data\n",
        "4. values is the number of samples in each class in the training data (sum of 'values' is the same as 'samples')\n",
        "\n",
        "**Examine the following:**\n",
        "1. What happens when we have more or less noise variables in the data? Do they make it into the tree?\n",
        "2. What if we make the tree bigger or smaller (n=1 or n=5)? How do you think the model will perform\n",
        "3. Try make the max_depth=7. Is the tree still interpretable? How many noise variables are included"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eediBVQSMfw8"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "dt_model <- models[['decision_tree']]\n",
        "\n",
        "# Fit the model\n",
        "dt_fit <- dt_model %>%\n",
        "  fit(diabetes ~ ., data = diabetes_train_df, model=T)\n",
        "\n",
        "# Plot the decision tree\n",
        "rpart.plot(dt_fit$fit, type = 3, extra = 104, cex = 0.8, fallen.leaves = TRUE, tweak = 1.2,\n",
        "           box.palette = \"RdYlGn\", shadow.col = \"gray\", nn = TRUE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lIl9OMtMgOP"
      },
      "source": [
        "## 2.3 Exploring different metrics\n",
        "\n",
        "We can also change the way that we evaluate model performance, again through the standardised interface provided by scikit-learn\n",
        "\n",
        "A list of possible options are provided can be see at\n",
        "https://yardstick.tidymodels.org/articles/metric-types.html\n",
        "\n",
        "Examine the following:\n",
        "\n",
        "1. Do different metrics ever change the ranking of which methods are best?\n",
        "2. How do results compare on the internal and external predictions? Are they the same? Do they dramatically differ?\n",
        "3. What happens when you adjust the amount of noise variables in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfcqElGeMjUb"
      },
      "outputs": [],
      "source": [
        "train_test_split_rset <- initial_split_to_rset(train_test_split, diabetes_df)\n",
        "metrics_dict <- metric_set(accuracy, bal_accuracy, roc_auc, mn_log_loss)\n",
        "\n",
        "multi_model_workflow <-  workflow_set(\n",
        "    preproc = list(\"formula\" = diabetes ~ .),\n",
        "    models = models\n",
        "  )\n",
        "\n",
        "# Fit models with resampling and pass options\n",
        "results <- multi_model_workflow %>%\n",
        "  workflow_map(\n",
        "    \"fit_resamples\",\n",
        "    resamples = train_test_split_rset,\n",
        "    metrics = metrics_dict,\n",
        "    control = control_resamples(save_pred = TRUE)\n",
        "  )\n",
        "\n",
        "# Collect the metrics\n",
        "metrics <- results %>%\n",
        "  collect_metrics(summarize=FALSE)\n",
        "\n",
        "# Print the performance table\n",
        "# NB: The pivoting here should actually be done automatically\n",
        "# in tune::collect_metrics but I couldn't get it working.\n",
        "metrics %>%\n",
        "  pivot_wider(id_cols=c(wflow_id,id),\n",
        "  names_from = `.metric`, values_from = `.estimate` ) %>%\n",
        "  filter(id=='Training') %>%\n",
        "  knitr::kable(digits=3, format='simple')\n",
        "\n",
        "metrics %>%\n",
        "  pivot_wider(id_cols=c(wflow_id,id), names_from = `.metric`, values_from = `.estimate` ) %>%\n",
        "  filter(id=='Testing') %>%\n",
        "  knitr::kable(digits=3, format='simple')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aMhXMt5Mj4a"
      },
      "source": [
        "# Session 3: Train/test and cross-validation frameworks\n",
        "\n",
        "In the previous example, we built model on entire dataset and evaluated its performance on the same data. Here, we will explore some alternative frameworks for doing this and will evaluate how model performance changes. We'll also start to explore different models and how key parameters can be altered to change prediction performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNT-bSTjMtYj"
      },
      "source": [
        "## 3.1 Training and Test split\n",
        "\n",
        "The following code examines training and testing a model on a single dataset and compares its performance to an external dataset in 3 scenarios:\n",
        " - Training on the entire dataset. Test on the same entire dataset.\n",
        " - Training on a proportion (default 80%). Test on the same proportion.\n",
        " - Training on a proportion (default 80%). Test on the remaining proportion.\n",
        "\n",
        "**Questions:**\n",
        "1. Run this cell a few times. Which accuracies change? Why?\n",
        "2. Which evaluation scenario is closest to the external data performance?\n",
        "2. What is the best performance you observe without changing parameters? What is the worst performance you observe?\n",
        "3. What happens to test performance as you add more noise variables?\n",
        "4. Try changing parameters ('C' for the l2 penalized logistic regression, or max_depth for Random Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJHbLG6Cu6VT"
      },
      "outputs": [],
      "source": [
        "# Setup three types of splits\n",
        "# 1 Where we train on everything\n",
        "# 2 Where we train on the trianing data and test on the training data\n",
        "# 3 Where we train on the training data and test on the test data\n",
        "train_test_split_rset <- initial_split_to_rset(train_test_split, diabetes_df, all=T, df2=diabetes_study2_df)\n",
        "\n",
        "metrics_dict <- metric_set(roc_auc)\n",
        "\n",
        "multi_model_workflow <-  workflow_set(\n",
        "    preproc = list(\"formula\" = diabetes ~ .),\n",
        "    models = models\n",
        "  )\n",
        "\n",
        "# Fit models with resampling and pass options\n",
        "results <- multi_model_workflow %>%\n",
        "  workflow_map(\n",
        "    \"fit_resamples\",\n",
        "    resamples = train_test_split_rset,\n",
        "    metrics = metrics_dict,\n",
        "    control = control_resamples(save_pred = TRUE)\n",
        "  )\n",
        "\n",
        "# Collect the metrics\n",
        "metrics <- results %>%\n",
        "  tune::collect_metrics(summarize=FALSE)\n",
        "\n",
        "metrics %>%\n",
        "  ggplot(aes(x=id, y=.estimate, fill=model)) +\n",
        "  geom_col(position='dodge') +\n",
        "    theme_light(base_size=22) +\n",
        "  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=0.5)) +\n",
        "  ylab(\"AUC\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSek4SIkM0J7"
      },
      "source": [
        "## 3.2 K-fold Cross-validation\n",
        "Variability in the performance of different splits in the previous example motivates the use of K-fold cross validation. Here, we explore a few models and start to compare model performance.  \n",
        "\n",
        "***Warning***: Be careful setting the values below. Setting the number of times to evaluate the classifiers too high and it will take too long to run for this workshop.\n",
        "\n",
        "**Questions:**\n",
        "1. Run this cell a few times. What is the range of the scores that are observed?\n",
        "2. Which model is the best? How do you determine this?\n",
        "3. Play around with hyperparameters, what is the impact on model performance? Which models are sensitive to these choices?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IksMzPKlMzy3"
      },
      "outputs": [],
      "source": [
        "metrics_dict <- metric_set(roc_auc)\n",
        "\n",
        "multi_model_workflow <-  workflow_set(\n",
        "    preproc = list(\"formula\" = diabetes ~ .),\n",
        "    models = models\n",
        "  )\n",
        "\n",
        "# K-fold cross-validation\n",
        "kfold=vfold_cv(diabetes_df, v = 3, repeats = 2)\n",
        "\n",
        "# Fit models with resampling and pass options\n",
        "results <- multi_model_workflow %>%\n",
        "  workflow_map(\n",
        "    \"fit_resamples\",\n",
        "    resamples = kfold,\n",
        "    metrics = metrics_dict,\n",
        "    control = control_resamples(save_pred = TRUE)\n",
        "  )\n",
        "\n",
        "# Collect the metrics\n",
        "metrics <- results %>%\n",
        "  tune::collect_metrics(summarize=FALSE)\n",
        "\n",
        "metrics %>%\n",
        "  ggplot(aes(x=model, y=.estimate, fill=model)) +\n",
        "  geom_boxplot(position='dodge') +\n",
        "  ylab(\"AUC\") + theme_light(base_size=22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9dGzxW1M6vj"
      },
      "source": [
        "# Session 4. Overfitting via feature selection and model parameters\n",
        "\n",
        "\n",
        "## 4.1 Feature discrimination over entire dataset\n",
        "One naive way to remove noise is to look at the features one-by-one,  look at their ability to discriminate the dataset and only take the most useful into our model. This is flawed but is common in the literature.\n",
        "\n",
        "Lets take a look at the discriminatory ability of our features, here using significance from a logistic regression. We report the coefficient (as a measure of effect size) and p-value for each features.\n",
        "\n",
        "**Question:**\n",
        "1. How do measured and noise features compare?\n",
        "2. What if we generate lots (n=10,000) noise features? How often can we distinguish noise and real signal?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sapply(setdiff(colnames(diabetes_df), 'diabetes'), \\(nm) {\n",
        "  res=coef(summary(glm(str_c('diabetes~',nm), family = 'binomial', data = diabetes_df)));\n",
        "  data.frame(feature=nm, beta=res[1,4], pvalue=res[2,4])},\n",
        "simplify = F ) %>%\n",
        "bind_rows() %>%\n",
        "arrange(pvalue) %>%\n",
        "slice_head(n=10)"
      ],
      "metadata": {
        "id": "yXXwl4K5qui2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TXTWgBWM-Rg"
      },
      "source": [
        "## 4.2 Demonstrating potential overfitting when selecting features before CV\n",
        "Given we've ranked the features in terms of their discrinatory ability, we could now select some top amount (based on p-value, f-statistic or a feeling for how many features we need).\n",
        "\n",
        "But such an approach uses all of the information, and hence means there is no unsed data left for an untouched test set.\n",
        "\n",
        "To explore this impact, the code below plots classifier performance starting with a single most discrinimatory feature and increasing to the top 32 features. We plot the model performance in training and testing.\n",
        "\n",
        "Additionally, we plot the performance of constructing a model on all samples and evaluating the external dataset to show where the ideal would be.\n",
        "\n",
        "**Questions**\n",
        "1. What are the trends in the performance of the model on the training data as we increase features?\n",
        "2. What are the trends in the performance of the model on the test data as we increase features?\n",
        "3. Where is the ideal number of features for the external data?\n",
        "4. How often does number of features to achieve the highest \"test\" performance correspond to the hihgest external performance?\n",
        "5. What happens if you run this cell a few times? How do results differ? Why do they change?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tHs6m_CXqvPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOKOqmH6NDuY"
      },
      "source": [
        "## 4.3 Nested cross-validation for feature selection and hyperparameter tuning\n",
        "\n",
        "Rather than selecting the number of features to be included in a model (based on a random guess, or worse peeking at test results), we can instead conduct feature selection as part of cross validation.\n",
        "\n",
        "The code below runs two cross-validation loops (inner and outer), essentially running one loop on the training data (repeatededly breaking it into training and validation datasets) to understand how the number of features impacts performance. We then select the best number of features and evaluate the held-out testset. This is then repeated for the number of folds in the outer loop.\n",
        "\n",
        "While robust, the approach can be computationally expensive as we are building many models.\n",
        "\n",
        "**Warning** This code will take a few minutes. If you add in hyperparameter selection (by uncommenting param_grid),  this could take quite a while to run in Google Colab.\n",
        "\n",
        "**Questions:**\n",
        "1. Try generating a dataset with no noisy features and one with many? How much does performance vary?\n",
        "2. How does performance vary if we change from a penalized regression to a random forest?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HiTimjgNIUH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define models with some parameters for tuning\n",
        "models <- list()\n",
        "models[['log_reg']] <- logistic_reg(mode = \"classification\", penalty = tune()) %>%\n",
        "  set_engine(\"glm\")\n",
        "\n",
        "models[['lasso']] <- logistic_reg(penalty = tune(), mixture = 1) %>%\n",
        "  set_engine(\"glmnet\")\n",
        "\n",
        "models[['rf']] <- rand_forest(mtry = tune(), trees = 500, min_n = tune()) %>%\n",
        "  set_engine(\"randomForest\")\n",
        "\n",
        "models[['decision_tree']] <- decision_tree(tree_depth = tune()) %>%\n",
        "  set_engine(\"rpart\")\n",
        "\n",
        "# Define the tuning grids\n",
        "log_reg_grid <- grid_regular(penalty(), levels = 10)\n",
        "lasso_grid <- grid_regular(penalty(), levels = 10)\n",
        "rf_grid <- grid_regular(mtry(range = c(1, 5)), min_n(range = c(1, 5)), levels = 10)\n",
        "decision_tree_grid <- grid_regular(tree_depth(), levels = 10)\n",
        "\n",
        "d cross-validation\n",
        "kfold <- vfold_cv(diabetes_df, v = 3, repeats = 2)\n",
        "\n",
        "# Fit models with tuning and pass options\n",
        "results <- multi_model_workflow %>%\n",
        "  workflow_map(\n",
        "    \"tune_grid\",\n",
        "    resamples = kfold,\n",
        "    grid = list(\n",
        "      log_reg = log_reg_grid,\n",
        "      lasso = lasso_grid,\n",
        "      rf = rf_grid,\n",
        "      decision_tree = decision_tree_grid\n",
        "    ),\n",
        "    metrics = metrics_dict,\n",
        "    control = control_grid(save_pred = TRUE)\n",
        "  )\n",
        "\n",
        "# Collect the metrics\n",
        "metrics <- results %>%\n",
        "  tune::collect_metrics(summarize = FALSE)\n",
        "\n",
        "# Plot the metrics\n",
        "metrics %>%\n",
        "  ggplot(aes(x = wflow_id, y = .estimate, fill = wflow_id)) +\n",
        "  geom_boxplot(position = \"dodge\") +\n",
        "  ylab(\"AUC\") +\n",
        "  theme_light(base_size = 22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Fepu7yNHL1"
      },
      "source": [
        "## Final task\n",
        "Using the above code, can you implement a scheme to answer the question \"do blood serum markers help predict diabetes progression beyond age, sex, bmi and blood glucose?\".\n",
        "\n",
        "Steps:\n",
        " - Copy the code in the previous section\n",
        " - Add call to a model with a specified feature subset (as in section Cell 1.3)\n",
        " - Compare boxplots from these two models"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}