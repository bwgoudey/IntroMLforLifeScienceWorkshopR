---
title: "Intro to Machine Learning For LIfe Sciences"
author: "Benjamin Goudey"
date: "2024-04-16"
output: html_document
---

```{r setup, include=FALSE}
# Load packages
#
library(rpart.plot)
library(tidyverse)
library(MASS)  # for mvnorm (multivariate normal data generation)
library(scales)  # for rescaling data'
library(remotes)
library(tidymodels)
library(DataExplorer)
library(pROC)

## Read in a set of helper functions
# Normally you'd just hve these locally but because we are using Google Colab, I found it
# easier to read these in directly from github.
source('helpers.R')
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
# Reand the dataset and make sure it's similar to sklearn's format
#source('./helpers.R')
devtools::source_url('https://raw.githubusercontent.com/bwgoudey/IntroMLforLifeScienceWorkshopR/main/helpers.R')
```

## Including Plots

You can also embed plots, for example:
### Load data
```{r pressure, echo=FALSE}
diabetes_df_raw <- read.csv("./RC_health_data_n2000.csv") %>% filter(!is.na(id))

diabetes_data <- load_diabetes_data(diabetes_df_raw, nsamples=2000, add_n_features=1000) 

diabetes_study1_df <- diabetes_data[['study1']]
diabetes_study2_df <- diabetes_data[['study2']]

```
### Explored data
```{r report}
DataExplorer::plot_histogram()
```

```{r}
plot_roc_auc <- function(yp_train_df, yp_test_df, y_train, y_test, label, base_size=25) {

  preds <- yp_train_df %>%
      bind_cols(y_train) %>%
      mutate(subset="train") %>%
    rbind(
      yp_test_df %>%
      bind_cols(y_test) %>%
      mutate(subset="test")
    ) %>%
  mutate(subset=factor(subset, c("train", "test")))

  auc_df <- preds %>%
    group_by(subset) %>%
      yardstick::roc_auc(truth = diabetes, .pred_0)  %>%
        rename(auc=.estimate)

  roc_curve_df <- preds %>%
    group_by(subset) %>%
      yardstick::roc_curve(truth = diabetes, .pred_0)

  metrics_df = roc_curve_df %>%
    left_join(auc_df, by='subset')

  # Plot training and external validation ROC curves
  metrics_df %>%
    ggplot(aes(x=1-specificity, y=sensitivity, color=subset)) +
    geom_line() +
    labs(title = paste(label, "ROC Curve"),
         x = "False Positive Rate (1-Specificity)",
         y = "True Positive Rate (Sensitivity)") +
    theme_light(base_size = base_size) +
    geom_text(
      aes(x = 0.6, y = 0.3, label = paste("AUC =", round(auc,3))),
      size = base_size/2,  inherit.aes = FALSE
    ) +  facet_wrap(~subset, nrow=1) + theme(legend.position = 'none')
}

```
## Ex 1
### Fitting a model
```{r}

diabetes_data <- load_diabetes_data(diabetes_df_raw, nsamples=400, add_n_features=10)
diabetes_study1_df <- diabetes_data[['study1']]
diabetes_study2_df <- diabetes_data[['study2']]

# Setting up the model specification
logit_spec <- logistic_reg(mode = "classification", penalty = 0, engine = "glm")

# Setting up the workflow
workflow <- workflow() %>%
  add_model(logit_spec) %>%
  add_formula(diabetes ~ .) # Any variable on the left-hand side of the tilde (~) is considered the model outcome (here, arr_delay). On the right-hand side of the tilde are the predictors. Variables may be listed by name, or you can use the dot (.) to indicate all other variables as predictors.

# Fitting the model
model_fit <- workflow %>%
         fit(data = diabetes_data[['study1']])

# Make predictions
diabetes_train_preds <- predict(model_fit, diabetes_data[['study1']], type = "prob")
diabetes_test_preds  <- predict(model_fit, diabetes_data[['study2']], type = "prob")

# Pull out the actual labels
diabetes_train_labels <- diabetes_data[['study1']] %>% dplyr::select(diabetes)
diabetes_test_labels  <- diabetes_data[['study2']] %>% dplyr::select(diabetes)


# Now plot the ROC curves and their AUCs
plot_roc_auc(diabetes_train_preds, diabetes_test_preds,
             diabetes_train_labels, diabetes_test_labels,
             label="Diabetes - Logistic Regression", base_size = 20)

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
## Ex 2
### Compare different models
```{r}

# Define the models
models=list()
models[['log_reg']]=logistic_reg(mode = "classification", penalty = 0) %>% set_engine("glm")
models[['lasso']]=logistic_reg(penalty = 0.1, mixture = 1) %>% set_engine("glmnet")
models[['rf']]=rand_forest(mtry = 2, trees = 500, min_n = 3) %>% set_engine("randomForest")
models[['decision_tree']]=decision_tree(tree_depth = 5) %>% set_engine("rpart")

models = sapply(models, \(m) m %>% set_mode("classification"), simplify = F, USE.NAMES = T)

# Fit the model of interest
#Change this model 
current_model=models[["log_reg"]] %>% set_mode("classification")

model_fit <- current_model %>% 
  fit(diabetes ~ ., data = diabetes_data[['study1']])

# Get predicted labels from the model
yp_training <- predict(model_fit, diabetes_data[['study1']], type = "prob")$.pred_1
yp_testing <- predict(model_fit, diabetes_data[['study2']], type = "prob")$.pred_1

p1 <- plot_roc(y = as.numeric(diabetes_data[['study1']]$diabetes)-1, yp=yp_training, "Training")
p2 <- plot_roc(y = as.numeric(diabetes_data[['study2']]$diabetes)-1, yp=yp_testing, "Testing")

# Combine plots
p1 + p2

```

### Specific model output
```{r}
library(rpart.plot)

# Define the model
dt_model <- models[['decision_tree']] 

# Fit the model
dt_fit <- dt_model %>%
  fit(diabetes ~ ., data = diabetes_data[['study1']], model=T)

# Plot the decision tree
rpart.plot(dt_fit$fit, type = 3, extra = 104, cex = 0.8, fallen.leaves = TRUE, tweak = 1.2, 
           box.palette = "RdYlGn", shadow.col = "gray", nn = TRUE)
```
### Test multiple metrics
```{r}
train_test_split_rset <- df_to_rset(diabetes_df[['study1']], diabetes_df[['study2']], all=F)


train_test_split_rset <- initial_split_to_rset(train_test_split, diabetes_data[['study1']])
metrics_dict <- metric_set(accuracy, bal_accuracy, roc_auc, mn_log_loss)

multi_model_workflow <-  workflow_set(
    preproc = list("formula" = diabetes ~ .),
    models = models
  )

# Fit models with resampling and pass options
results <- multi_model_workflow %>%
  workflow_map(
    "fit_resamples",
    resamples = train_test_split_rset,
    metrics = metrics_dict,
    control = control_resamples(save_pred = TRUE)
  )

# Collect the metrics
metrics <- results %>%
  tune::collect_metrics(summarize=FALSE)

# Print the performance table
metrics %>% 
  pivot_wider(id_cols=c(wflow_id,id), names_from = `.metric`, values_from = `.estimate` ) %>% filter(id=='Training')%>% knitr::kable(digits=3, format='html')

metrics %>% 
  pivot_wider(id_cols=c(wflow_id,id), names_from = `.metric`, values_from = `.estimate` ) %>% filter(id=='Testing') %>% knitr::kable(digits=3, format='html')


```



# external eval
```{r}
#diabetes_data <- load_diabetes_data(diabetes_df_raw, nsamples=400, add_n_features=10)

# Setup three types of splits
# 1 Where we train on everything
# 2 Where we train on the trianing data and test on the training data
# 3 Where we train on the training data and test on the test data

# Split study 1 into 80% for training, 20% for testing
train_test_split=initial_split(diabetes_data[['study1']], prop = 0.8)
train_test_split_rset <- initial_split_to_rset(train_test_split, diabetes_data[['study1']], all=T, df2=diabetes_data[['study2']])

metrics_dict <- metric_set(roc_auc)

multi_model_workflow <-  workflow_set(
    preproc = list("formula" = diabetes ~ .),
    models = models
  )

# Fit models with resampling and pass options
results <- multi_model_workflow %>%
  workflow_map(
    "fit_resamples",
    resamples = train_test_split_rset,
    metrics = metrics_dict,
    control = control_resamples(save_pred = TRUE)
  )

# Collect the metrics
metrics <- results %>%
  tune::collect_metrics(summarize=FALSE)

metrics %>%
  ggplot(aes(x=id, y=.estimate, fill=model)) +
  geom_col(position='dodge') +
    theme_light(base_size=22) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=0.5)) +
  ylab("AUC")
```

```{r}
metrics_dict <- metric_set(accuracy, bal_accuracy, roc_auc, mn_log_loss)

# Define models with some parameters for tuning
models_tuned <- list()
models_tuned[['log_reg_tune']] <- logistic_reg(mode = "classification") %>%
  set_engine("glm")

models_tuned[['lasso_tune']] <- logistic_reg(mode = "classification",penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

models_tuned[['rf_tune']] <- rand_forest(mode = "classification",mtry = 1, trees = 500, min_n = tune()) %>%
  set_engine("randomForest")

models_tuned[['decision_tree_tune']] <- decision_tree(mode = "classification",tree_depth = tune()) %>%
  set_engine("rpart")


# Define the tuning grids
log_reg_grid <- grid_regular(penalty(), levels = 10)
lasso_grid <- grid_regular(penalty(), levels = 10)

#d cross-validation
kfold <- vfold_cv(diabetes_data[['study1']], v = 3, repeats = 2)

multi_model_workflow <-  workflow_set(
    preproc = list("formula" = diabetes ~ .),
    models = c(models,models_tuned)
  )
# Fit models with tuning and pass options
results <- multi_model_workflow %>%
  workflow_map(
    "tune_grid",
    resamples = kfold,
    grid = 5,
    metrics = metrics_dict,
    control = control_grid(save_pred = TRUE)
  )

# Collect the metrics
metrics <- results %>%
 tune::collect_metrics(summarize = T)

autoplot(results, metric = "roc_auc") + theme_light(base_size=15)


autoplot(results, metric = "roc_auc", id = 'formula_lasso_tune') + theme_light(base_size=15)

```



